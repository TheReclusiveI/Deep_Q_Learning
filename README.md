# Deep\_Q\_Learning
## 大致介绍
这个项目的来源是前不久的两篇论文，Playing Atari with Deep Reinforcement Learning，和DeepMind-DQN-nature。论文中，作者使用强化学习的方法让人工智能学会了打游戏，包括atria中的多款游戏。有部分的游戏效果很好，也有部分的效果不好，这个代码的用处就是复现了一下使用CNN和强化学习来训练agent来玩太空入侵者这款游戏。
![](https://github.com/gwyxjtu/Deep_Q_Learning/blob/master/img/dqn.gif)
## 前期的准备工作
### 安装gym
首先得安装gym中的atari部分，如果有错误，可以参考我之前写的博客
    [传送门](https://blog.csdn.net/weixin_42578412/article/details/89790172 "传送门")
### 安装opencv
opencv是用来对图像进行处理的，可以省下很多功夫。

可以直接`pip install opencv-python`
### 安装keras
keras类似于pytorch，构建网络十分方便。但是前提是已经安装好tensorflow。
## 论文中数学思想
这里以论文为基础，讲一下我的理解，难免会错误，还请大佬们提出。
### Playing Atari with Deep Reinforcement Learning
+ 这篇论文可以解决的问题
	+	 大多数深度学习都依靠大量的手工数据集标记
	+	深度学习的样本都是独立的，RL中确实有记忆性的，前后的状态会互相影响。
	+	深度学习的目标分布是固定的，强化学习的目标不固定，比如游戏往后面会切换场景，就得分开训练或者切换模型。
+ 模拟器的内部是不能被agent观察的，观察的是图片。
+ agent的目的是与模拟器交互，选择reward最大的action
+ 与TD-Gammon和类似的在线方法形成对比，我们使用一种被称为经验重放的技术，我们在每一个时间步长中存储agent的经验，把每一步的经验存储在数据集中。在算法的内部循环中，我们从存储的样本池中随机抽取e ∼ D，并用Q-learning更新或小批量更新。在执行 经验重放 后，agent根据 贪婪策略 选择并执行一个action。由于使用任意长度的历史作为神经网络的输入可能很困难，取而代之，我们的q函数作用于固定长度的表示，这个表示是由 函数 提供的。算法1给出了完整算法，我们称之为深度Q-learning。
+ 这种方法比标准的在线Q-learning[23]有几个优点。
	+ 首先，经验的每一步都有可能用于许多权重更新，从而提高数据效率。
	+ 第二，直接从连续的样本中学习效率很低，因为样本之间的相关性很强；随机化样本打破了这些相关性，因此减少了更新的方差。
	+ 第三，当在策略上学习时，当前参数决定了下一个参数所训练的数据样本。例如，如果函数值最大的动作是左移，则训练样本将由来自左手侧的样本主导；如果最大化的动作切换到右边，那么训练的分布也会改变。因此，可能出现不希望有的反馈回路，且这些参数可能会陷入一种较差的局部最小值，或者甚至是灾难性的。通过使用经验重放，行为分布在它以前的许多状态上是平滑的，避免参数的振荡或发散。
+ 原始的Atari框架（210*160像素，128个颜色）对计算要求太高，因此，我们应用了一个预处理步骤来降低输入的维数。原始帧被预处理，首先将它们的RGB转换成灰度，并将其降采样到110*84的图像。最后的输入：裁剪成一个84*84的图像区域，这个区域大致可以捕捉到游戏区域。最后的裁剪阶段是必需的，因为我们使用的是2D卷积的GPU实现，它期望的是方形输入。对于本文的实验，算法1的函数将此预处理应用于历史的最后4帧，并将它们堆叠起来生成q函数的输入。
+ 存在几种“用神经网络将Q进行参数化”的方式。由于Q将历史-动作对映射到它们的Q值，历史和动作在以前一些方法已经被用作神经网络的输入。这种架构的主要缺点是：需要单独的前向传递来计算每个操作的q值，这导致计算成本与操作的数量成线性关系。输出对应于输入状态下单个动作的预测值。这种架构的主要优点是：只通过一个前向网络，能够计算给定状态下所有可能动作的q值。我们现在描述了所有7款雅达利游戏的架构。神经网络的输入是一个84*84*4的图像。第一个隐藏层：16个8 × 8 filters with stride 4 ，并进行非线性处理。第二个隐藏层：卷积了32个4 × 4 filters with stride 2，并进行非线性处理。最后的隐藏层是全连接的，由256个整流单元组成。输出层是一个全连接的线性层，对于每个有效动作只有一个输出。在我们考虑的游戏中，有效动作的数量在4到18之间变化。我们将使用我们的方法训练的卷积网络称为深度q网络(Deep Q-Networks, DQN)。
+ 按照以前的方法来玩Atari游戏，我们也使用了一个简单的帧跳过技术。更准确的说，智能体每k帧选择一次，它的最后一个动作在跳过的帧上重复。由于在一个步骤上运行模拟器需要比让智能体选择一个动作要少得多的计算，这种技术允许智能体在不显著增加运行时的情况下，玩大约k次的游戏。我们在所有游戏中都使用k = 4，除了《太空入侵者》，我们注意到使用k = 4会使激光看不见，我们用k = 3使激光可见，这个变化是所有游戏中超参数值的唯一区别。
+ 在这些实验中，我们使用了RMSProp算法。mini-batch大小为32。在训练过程中，行为策略是贪婪的，从1到0.1线性退火，在第一百万帧中，此后固定为0.1.我们总共训练了1000万帧，并使用了100万张最新帧的重放记忆。
+ 双重DQN的思想是，在建立一个模型的同时，建立一个有延迟的模型用来进行验证，因为如果使用同一个模型来尽心预测和验证的话，说服力会更小。
###  Q-Learning算法

Q-Learning算法下，目标是达到目标状态(Goal State)并获取最高收益，一旦到达目标状态，最终收益保持不变。因此，目标状态又称之为吸收态。

通常，我们需要构建一个即时奖励矩阵R，用于表示从状态s到下一个状态s’的动作奖励值。
由即时奖励矩阵R计算得出指导agent行动的Q矩阵。

$Q(s,a)=R(s,a)+\gamma max[Q(s',all action)]$

其中$s'$表示下一个状态。通过这个推到公式可以计算出Q矩阵的元素。
#### 一些参数
+ s:当前的状态
+ a:当前的行动
+ s':行动后产生的新一轮state
+ a':下次的action
+ r:本次行动的奖励
+ $\gamma$:折扣因数，表示牺牲当前收益，换取长远收益的程度。
#### 算法思想
+ 设置$\gamma$以及矩阵$R$
+ 初始化Q矩阵全部为0
+ for each episode:
	+ 随机选择一个初始状态
	+ while(最终目标没有达成):
		+ 从当前可能的action中选择一个
		+ 执行到下一个state
		+ 找到下一个状态中的$Q_{max}$
		+ 计算$Q(s,a)=R(s,a)+\gamma max[Q(s',all action)]$
		+ 将下一次的state设置为当前的state
	+ end while
+ end for

每次学习后Q矩阵都会更新，多次训练后可以得到训练好的Q矩阵，之后就是使用Q矩阵，就是每次都从其中选择奖励最大的值就可以

### 论文中的Q-Learning
### reward的表达式
$R_t=\sum_{t'=t}^T\gamma^{t'-t}r_{t'}$
从公式中可以看出来，reward是将未来T-t个时刻的reward加在了一起，T是游戏终止的时刻。
### Q值的表达式(最优价值函数)
$Q^*(s,a)= max_\pi\mathbb{E}[R_t|s_t=s,a_t=a,\pi]$
这个是期望（找了半天才发现是期望），$\pi$是一个策略，能够从s映射到a。
最优值Q(s’, a’)在不同的a’的选择下都知道了，最优的策略就是选择能够使下式最大的a’。
$$Q^*(s,a)= \mathbb{E}_{s'\xi}[r+\gamma \max_{a'}Q^*(s',a')|s,a]$$
许多强化学习算法背后的基本思想是：通过使用Bellman方程作为迭代更新，来估计action-value函数的值。
$$Q_{i+1}(s,a)= \mathbb{E}[r+\gamma \max_{a'}Q^*(s',a')|s,a]$$
这个时候
$$Q_i\rightarrow Q^*\quad as\quad i\rightarrow\infin$$
但是在实践中，这种基本方法是完全不切实际的，因为每个序列的action-value函数都是单独估计的，没有任何一般化。取而代之，通常使用“函数逼近器”来估计action-value函数，$Q(s,a,\theta)\approx Q^*(s,a)$在强化学习中，这通常是一个线性函数逼近器，如果是非线性的逼近就是神经网络。
我们将权值为$\theta$的神经网络函数逼近器称为Q-network。
### 损失函数
$$L_i(\theta _i)= \mathbb{E}_{s,a}[(y_i-Q(s,a;\theta_i))^2]$$
其中$y_i$就是第i轮迭代的目标真实值，之后进行随机梯度下降函数进行优化。

## 详细的思路（画一个流程图）
![](https://github.com/gwyxjtu/Deep_Q_Learning/blob/master/img/CNN.PNG)
## 代码讲解
之前写过一篇博客讲解相关的代码，具体可以参考

[传送门](https://blog.csdn.net/weixin_42578412/article/details/89931766)
## 具体的分工
郭主要进行gym相关接口学习，reward奖励策略等。孙主要负责CNN的搭建以及调参，Qlearning算法学习机制等。李主要负责tensorflow版本的编写，从底层入手学习这个算法。
## 代码使用方法
### 换游戏的方法
在代码的124行，可以更改游戏的名字
### 具体的参数
在DQNAgent的类里面，可以自行调整连接层的参数
### 模型保存和读取
load和save，不多说了


## 代码不足以及之后改进的地方
